{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use llama (nomic_embed-text) in Docker to extract embeddings\n",
    "Step 1: Start the Ollama Docker container (if not already running)\n",
    "\n",
    "docker run -d \\\n",
    "  --gpus all \\\n",
    "  --name ollama \\\n",
    "  -v ~/.ollama:/root/.ollama \\\n",
    "  -p 11434:11434 \\\n",
    "  ollama/ollama\n",
    "\n",
    "Pull the embedding model (e.g., nomic-embed-text)\n",
    "\n",
    "docker exec -it ollama ollama pull nomic-embed-text\n",
    "\n",
    "Step 2: run the following code plock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Load CSV file\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "sentences = df[\"text\"].tolist()  # Replace \"text\" with your column name\n",
    "\n",
    "# Ollama API endpoint\n",
    "OLLAMA_HOST = \"http://localhost:11434\"\n",
    "MODEL_NAME = \"nomic-embed-text\"  # Or any embedding model (e.g., \"llama2\")\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = []\n",
    "for sentence in sentences:\n",
    "    response = requests.post(\n",
    "        f\"{OLLAMA_HOST}/api/embeddings\",\n",
    "        json={\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"prompt\": sentence\n",
    "        }\n",
    "    )\n",
    "    if response.status_code == 200:\n",
    "        embedding = response.json()[\"embedding\"]  # Typo in Ollama's response? Double-check the key!\n",
    "        embeddings.append(embedding)\n",
    "    else:\n",
    "        print(f\"Failed for: {sentence}\")\n",
    "        embeddings.append(None)\n",
    "\n",
    "# Add embeddings to DataFrame\n",
    "df[\"embedding\"] = embeddings\n",
    "\n",
    "# Save to new CSV\n",
    "df.to_csv(\"data_with_embeddings-ollama.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use deepseek in Docker to extract embeddings\n",
    "Step 1: Start Ollama in Docker\n",
    "\n",
    "Start Ollama with GPU support\n",
    "\n",
    "docker run -d \\\n",
    "  --gpus all \\\n",
    "  --name ollama \\\n",
    "  -v ~/.ollama:/root/.ollama \\\n",
    "  -p 11434:11434 \\\n",
    "  ollama/ollama\n",
    "\n",
    "Pull your desired model (e.g., DeepSeek):\n",
    "\n",
    "docker exec -it ollama ollama pull deepseek-r1:8b\n",
    "\n",
    "tep 2: Then run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Load CSV data\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "sentences = df[\"text\"].tolist()\n",
    "\n",
    "# Ollama API endpoint\n",
    "OLLAMA_HOST = \"http://localhost:11434\"\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = []\n",
    "for sentence in sentences:\n",
    "    response = requests.post(\n",
    "        f\"{OLLAMA_HOST}/api/embeddings\",\n",
    "        json={\n",
    "            \"model\": \"deepseek-r1:8b\",  # Your model name\n",
    "            \"prompt\": sentence\n",
    "        }\n",
    "    )\n",
    "    if response.status_code == 200:\n",
    "        embedding = response.json().get(\"embedding\")\n",
    "        embeddings.append(embedding)\n",
    "    else:\n",
    "        print(f\"Error: {response.text}\")\n",
    "        embeddings.append(None)\n",
    "\n",
    "# Save results\n",
    "df[\"embedding\"] = embeddings\n",
    "df.to_csv(\"data_with_embedding_deepseek.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOT USE: Get Embeddings From Deepseek R1 With vLLM\n",
    "source: https://www.youtube.com/watch?v=OMzZ5SABapA\n",
    "- Note that DeepSeek currently does not offer any embeddings models, this method is not validated.\n",
    "- Current server's CUDA 11.4 is outdated: PyTorch 1.10.0 + CUDA 11.3 will not work with modern libraries like vLLM.\n",
    "    - Solution: need admin to upgrade CUDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-20 11:31:23 __init__.py:207] Automatically detected platform cuda.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "/home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/neuronlp/lib/python3.10/site-packages/vllm/_C.abi3.so: undefined symbol: cuTensorMapEncodeTiled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m sentences \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()  \u001b[38;5;66;03m# Replace \"text\" with your column name\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLM\n\u001b[0;32m----> 8\u001b[0m modelds \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdeepseek-ai/DeepSeek-R1-Distill-Llama-8B\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43membed\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43menforce_eager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39membed(sentences)\n",
      "File \u001b[0;32m~/anaconda3/envs/jupyter_env/envs/neuronlp/lib/python3.10/site-packages/vllm/utils.py:1022\u001b[0m, in \u001b[0;36mdeprecate_args.<locals>.wrapper.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1015\u001b[0m             msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1017\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1018\u001b[0m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[1;32m   1019\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[1;32m   1020\u001b[0m         )\n\u001b[0;32m-> 1022\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/jupyter_env/envs/neuronlp/lib/python3.10/site-packages/vllm/entrypoints/llm.py:242\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_overrides, mm_processor_kwargs, task, override_pooler_config, compilation_config, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# Logic to switch between engines is done at runtime instead of import\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# to avoid import order issues\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_engine_class()\n\u001b[0;32m--> 242\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUsageContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m~/anaconda3/envs/jupyter_env/envs/neuronlp/lib/python3.10/site-packages/vllm/engine/llm_engine.py:486\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates an LLM engine from the engine arguments.\"\"\"\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;66;03m# Create the engine configs.\u001b[39;00m\n\u001b[0;32m--> 486\u001b[0m engine_config \u001b[38;5;241m=\u001b[39m \u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_engine_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    487\u001b[0m executor_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_executor_cls(engine_config)\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# Create the LLM engine.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/jupyter_env/envs/neuronlp/lib/python3.10/site-packages/vllm/engine/arg_utils.py:1096\u001b[0m, in \u001b[0;36mEngineArgs.create_engine_config\u001b[0;34m(self, usage_context)\u001b[0m\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_engine_config\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1094\u001b[0m                          usage_context: Optional[UsageContext] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1095\u001b[0m                          ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VllmConfig:\n\u001b[0;32m-> 1096\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m current_platform\n\u001b[1;32m   1097\u001b[0m     current_platform\u001b[38;5;241m.\u001b[39mpre_register_and_update()\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m envs\u001b[38;5;241m.\u001b[39mVLLM_USE_V1:\n",
      "File \u001b[0;32m~/anaconda3/envs/jupyter_env/envs/neuronlp/lib/python3.10/site-packages/vllm/platforms/__init__.py:239\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _current_platform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    238\u001b[0m     platform_cls_qualname \u001b[38;5;241m=\u001b[39m resolve_current_platform_cls_qualname()\n\u001b[0;32m--> 239\u001b[0m     _current_platform \u001b[38;5;241m=\u001b[39m \u001b[43mresolve_obj_by_qualname\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mplatform_cls_qualname\u001b[49m\u001b[43m)\u001b[49m()\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mglobal\u001b[39;00m _init_trace\n\u001b[1;32m    242\u001b[0m     _init_trace \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(traceback\u001b[38;5;241m.\u001b[39mformat_stack())\n",
      "File \u001b[0;32m~/anaconda3/envs/jupyter_env/envs/neuronlp/lib/python3.10/site-packages/vllm/utils.py:1877\u001b[0m, in \u001b[0;36mresolve_obj_by_qualname\u001b[0;34m(qualname)\u001b[0m\n\u001b[1;32m   1873\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1874\u001b[0m \u001b[38;5;124;03mResolve an object by its fully qualified name.\u001b[39;00m\n\u001b[1;32m   1875\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1876\u001b[0m module_name, obj_name \u001b[38;5;241m=\u001b[39m qualname\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 1877\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1878\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, obj_name)\n",
      "File \u001b[0;32m~/anaconda3/envs/jupyter_env/envs/neuronlp/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/jupyter_env/envs/neuronlp/lib/python3.10/site-packages/vllm/platforms/cuda.py:15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParamSpec\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# import custom ops, trigger op registration\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01menvs\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init_logger\n",
      "\u001b[0;31mImportError\u001b[0m: /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/neuronlp/lib/python3.10/site-packages/vllm/_C.abi3.so: undefined symbol: cuTensorMapEncodeTiled"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "# Load CSV file\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "sentences = df[\"text\"].tolist()  # Replace \"text\" with your column name\n",
    "\n",
    "from vllm import LLM\n",
    "modelds = LLM(\n",
    "    model = 'deepseek-ai/DeepSeek-R1-Distill-Llama-8B',\n",
    "    task = 'embed',\n",
    "    enforce_eager = True\n",
    ")\n",
    "output = model.embed(sentences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
